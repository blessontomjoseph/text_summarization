{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install rouge_score\n","!pip install py7zr"]},{"cell_type":"markdown","metadata":{},"source":["### Work overview  (fine tuning a pretrained model for text summarization)\n","\n","- base_mdoel - google pegasus pretrained on cnn_dailymail(daily news articles and its summaries)\n","- fine tuned on -samsum(samsumng customer conversation data and its summaries)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import transformers\n","from transformers import AutoModelForSeq2SeqLM,AutoTokenizer,AutoModel\n","from datasets import load_dataset,load_metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["samsum_data = load_dataset(\"samsum\")\n","# samsum -customer conversations and its summaries\n","# cnn_dailymail -news articles and its summaries"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T09:37:29.960760Z","iopub.status.busy":"2023-05-13T09:37:29.960294Z","iopub.status.idle":"2023-05-13T09:51:20.620539Z","shell.execute_reply":"2023-05-13T09:51:20.619503Z","shell.execute_reply.started":"2023-05-13T09:37:29.960700Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c8c4602cebb4d0d8a99b4d8f55e5f2f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/205 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["class Config:\n","    device='cuda' if torch.cuda.is_available() else 'cpu'\n","    infer_batch_size=4\n","    model_checkpoint='google/pegasus-cnn_dailymail' #pretrained on cnn_dailymail\n","\n","class LoadData:\n","    def __init__(self,data,x,y,tokenizer=None):\n","        self.article=data[x]\n","        self.summary=data[y]\n","        self.tokenizer=tokenizer\n","    def __len__(self):\n","        return len(self.article)\n","    def __getitem__(self,idx):\n","        article=self.article.iloc[idx]\n","        summary=self.summary.iloc[idx]\n","        if self.tokenizer is not None:\n","            article=tokenizer(self.article.iloc[idx])\n","        return {'x':article,'y':summary}\n","\n","tokenizer=AutoTokenizer.from_pretrained(Config.model_checkpoint)\n","model=AutoModelForSeq2SeqLM.from_pretrained(Config.model_checkpoint).to(Config.device)\n","model=torch.nn.DataParallel(model)\n","rouge_metric=load_metric('rouge')\n","\n","def infer(data,model,tokenizer,metric):\n","    \"\"\"function tokenizes and computes the output of the data \n","    from the model,evaluates the score and returns it\"\"\"\n","    for batch in tqdm(data,total=len(data)):\n","        articles=batch['x']\n","        summaries=batch['y']\n","        tokens=tokenizer(articles,max_length=1024,padding='max_length',truncation=True,return_tensors='pt')\n","        out=model.module.generate(input_ids=tokens['input_ids'].to(Config.device),attention_mask=tokens['attention_mask'].to(Config.device),length_penalty=0.8,num_beams=8,max_length=128)\n","        pred_summaries=[tokenizer.decode(item,skip_special_tokens=True,clean_up_tokenization_spaces=True) for item in out]\n","        pred_summaries=[item.replace('<n>',\" \") for item in pred_summaries]\n","        metric.add_batch(predictions=pred_summaries,references=summaries) \n","        torch.cuda.empty_cache()\n","    score=metric.compute()\n","    return {i:score[i].mid.fmeasure for i in score.keys()}\n","\n","\n","# loading the test data and inferring on it to see the base performance\n","samsum_test=pd.DataFrame(samsum_data['test'])\n","samsum_test_loader=DataLoader(LoadData(samsum_test,x='dialogue',y='summary'),batch_size=Config.infer_batch_size,shuffle=False)\n","score_befor_fine_tuning=infer(samsum_test_loader,model,tokenizer,rouge_metric)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T09:53:09.904687Z","iopub.status.busy":"2023-05-13T09:53:09.904279Z","iopub.status.idle":"2023-05-13T09:53:17.906140Z","shell.execute_reply":"2023-05-13T09:53:17.905230Z","shell.execute_reply.started":"2023-05-13T09:53:09.904657Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"555332e07c8f4c36a545f9d48c4ffef0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"016b04dc86114c78854e76a0a1eb007d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc2dbcfe64824cf6983eb102b584c7b0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def make_tokens(example_batch):\n","    \"\"\"prepares tokens for training\"\"\"\n","    input_encodings=tokenizer(example_batch[\"dialogue\"],max_length=1024,truncation=True)    \n","    with tokenizer.as_target_tokenizer():\n","        target_encodings=tokenizer(example_batch[\"summary\"],max_length=128,truncation=True)\n","        \n","    return {\"input_ids\":input_encodings[\"input_ids\"], \n","            \"attention_mask\":input_encodings[\"attention_mask\"], \n","            \"labels\":target_encodings[\"input_ids\"]\n","           }\n","\n","samsum_tokens=samsum_data.map(make_tokens,batched=True)\n","columns=[\"input_ids\", \"labels\",\"attention_mask\"]\n","samsum_tokens.set_format(type=\"torch\",columns=columns)"]},{"cell_type":"markdown","metadata":{},"source":["#### training on huggingface Trainer class"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T09:53:49.776175Z","iopub.status.busy":"2023-05-13T09:53:49.775800Z","iopub.status.idle":"2023-05-13T10:35:10.676642Z","shell.execute_reply":"2023-05-13T10:35:10.675300Z","shell.execute_reply.started":"2023-05-13T09:53:49.776143Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.15.2 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230513_095356-2box6z5n</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/theothertom/huggingface/runs/2box6z5n' target=\"_blank\">effortless-snow-6</a></strong> to <a href='https://wandb.ai/theothertom/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/theothertom/huggingface' target=\"_blank\">https://wandb.ai/theothertom/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/theothertom/huggingface/runs/2box6z5n' target=\"_blank\">https://wandb.ai/theothertom/huggingface/runs/2box6z5n</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [920/920 32:58, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.695800</td>\n","      <td>1.483780</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6457add9d424219a0ec35d5f9956a1f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/205 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import DataCollatorForSeq2Seq,TrainingArguments,Trainer\n","\n","seq2seq_data_collator=DataCollatorForSeq2Seq(tokenizer,model=model.module)\n","training_args=TrainingArguments(output_dir='/kaggle/working/hf',\n","                                num_train_epochs=1,\n","                                warmup_steps=500,\n","                                per_device_train_batch_size=1, \n","                                per_device_eval_batch_size=1,\n","                                weight_decay=0.01, \n","                                logging_steps=10,\n","                                evaluation_strategy='steps',\n","                                eval_steps=500,\n","                                save_steps=1e6,\n","                                gradient_accumulation_steps=16)\n","\n","trainer=Trainer(model=model.module, \n","                  args=training_args,\n","                  tokenizer=tokenizer,\n","                  data_collator=seq2seq_data_collator,\n","                  train_dataset=samsum_tokens[\"train\"], \n","                  eval_dataset=samsum_tokens[\"validation\"]) \n","\n","trainer.train()\n","\n","#inferring on test data after fine tuning\n","score_after_fine_tuning=infer(data=samsum_test_loader,model=torch.nn.DataParallel(trainer.model),tokenizer=tokenizer,metric=rouge_metric)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T10:58:24.311907Z","iopub.status.busy":"2023-05-13T10:58:24.311403Z","iopub.status.idle":"2023-05-13T10:58:24.323833Z","shell.execute_reply":"2023-05-13T10:58:24.322794Z","shell.execute_reply.started":"2023-05-13T10:58:24.311865Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["scores_before_tuning: {'rouge1': 0.29624950139134154, 'rouge2': 0.08791421594725683, 'rougeL': 0.22920481105677556, 'rougeLsum': 0.22912836825557523} \n","\n","scores_after_tuning: {'rouge1': 0.43118236794877784, 'rouge2': 0.2004617211248804, 'rougeL': 0.3394828236863836, 'rougeLsum': 0.33964179236118636}\n"]}],"source":["print(f'scores_before_tuning: {score_befor_fine_tuning} \\n\\nscores_after_tuning: {score_after_fine_tuning}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
